{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# **Fraud Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "from typing import Any\n",
    "\n",
    "# Third Party Imports\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "# Internal Imports\n",
    "from fraud_detection.metrics import precision_at_K, recall_at_K, lift_at_K\n",
    "from fraud_detection.utils.plotting import (\n",
    "    plot_anomaly_score,\n",
    "    plot_precision_recall_curve,\n",
    "    plot_roc_curve,\n",
    ")\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## *Read data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = pl.read_csv(\n",
    "    r\"../data/input/creditcard.csv\",\n",
    "    ignore_errors=False,\n",
    "    infer_schema_length=1000_000,\n",
    ")\n",
    "\n",
    "# Display data\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# **Feature/Target & Train/Test/Validation Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.select(pl.exclude(\"Time\", \"Class\"))\n",
    "y = df.select(\"Class\").to_series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time based split\n",
    "## Train Data set\n",
    "X_train = X[: 2 * (X.height // 4)]\n",
    "y_train = y[: 2 * (y.len() // 4)]\n",
    "\n",
    "## Validation Data set\n",
    "X_val = X[2 * (X.height // 4) : 3 * (X.height // 4)]\n",
    "y_val = y[2 * (y.len() // 4) : 3 * (y.len() // 4)]\n",
    "\n",
    "# Test Data set\n",
    "X_test = X[3 * (X.height // 4) :]\n",
    "y_test = y[3 * (y.len() // 4) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display class distribution in training set\n",
    "display(\n",
    "    y_train.value_counts()\n",
    "    .sort(by=\"Class\")\n",
    "    .with_columns(\n",
    "        (pl.col(\"count\") * 100 / pl.col(\"count\").sum()).round(4).alias(\"percentage\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display class distribution in valid set\n",
    "display(\n",
    "    y_val.value_counts()\n",
    "    .sort(by=\"Class\")\n",
    "    .with_columns(\n",
    "        (pl.col(\"count\") * 100 / pl.col(\"count\").sum()).round(4).alias(\"percentage\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display class distribution in test set\n",
    "display(\n",
    "    y_test.value_counts()\n",
    "    .sort(by=\"Class\")\n",
    "    .with_columns(\n",
    "        (pl.col(\"count\") * 100 / pl.col(\"count\").sum()).round(4).alias(\"percentage\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# **Outlier Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## *Approach A â€” Isolation-Based Anomaly Detection*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only filter class 0 from the training data\n",
    "X_train_class_0 = X_train.filter(y_train == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ranking_metrics_at_k(\n",
    "    y_true: np.ndarray, y_pred_proba: np.ndarray, K_list: list[int]\n",
    ") -> pl.DataFrame:\n",
    "    # Has to be numpy array\n",
    "    if not isinstance(y_true, np.ndarray):\n",
    "        raise ValueError(\"y_true must be a numpy array\")\n",
    "    if not isinstance(y_pred_proba, np.ndarray):\n",
    "        raise ValueError(\"y_pred_proba must be a numpy array\")\n",
    "    # if not i Dimension array\n",
    "    if y_true.ndim != 1:\n",
    "        raise ValueError(\"y_true must be a 1D array\")\n",
    "    if y_pred_proba.ndim != 1:\n",
    "        raise ValueError(\"y_pred_proba must be a 1D array\")\n",
    "    # Constrains on K\n",
    "    if min(K_list) < 1:\n",
    "        raise ValueError(\"K must be greater than 0\")\n",
    "    if not all([isinstance(k, int) for k in K_list]):\n",
    "        raise ValueError(\"K must be an integer\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"K\": K_list,\n",
    "        \"Precision@K\": [precision_at_K(y_true, y_pred_proba, K) for K in K_list],\n",
    "        \"Recall@K\": [recall_at_K(y_true, y_pred_proba, K) for K in K_list],\n",
    "        \"Lift@K\": [lift_at_K(y_true, y_pred_proba, K) for K in K_list],\n",
    "    }\n",
    "    return pl.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third Party Imports\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Create the Isolation forest pipeline\n",
    "if_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", RobustScaler()),\n",
    "        (\"iso_forest\", IsolationForest(n_estimators=1_000, random_state=42, n_jobs=5)),\n",
    "    ]\n",
    ")\n",
    "# Train the Isolation Forest on Non-Fradulant Train data\n",
    "if_pipeline.fit(X_train_class_0)\n",
    "# Now get scores on the full training data\n",
    "anomaly_scores = -if_pipeline.decision_function(X_val)\n",
    "\n",
    "# Plot the Anomaly Scores\n",
    "plot_anomaly_score(anomaly_scores).show()\n",
    "\n",
    "# Plot the PR Curve\n",
    "_, if_threshold = plot_precision_recall_curve(y_val.to_numpy(), anomaly_scores)\n",
    "_.show()\n",
    "\n",
    "# Plot the ROC curve\n",
    "plot_roc_curve(y_val.to_numpy(), anomaly_scores).show()\n",
    "\n",
    "# Compute Average precision\n",
    "print(f\"Average Precision: {average_precision_score(y_val, anomaly_scores):.2%}\")\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_true=y_val, y_score=anomaly_scores):.2%}\")\n",
    "print(f\"Isolation Forest Threshold: {if_threshold}\")\n",
    "\n",
    "# Display the confusion matrix\n",
    "_ = ConfusionMatrixDisplay(\n",
    "    confusion_matrix(y_true=y_val, y_pred=anomaly_scores > if_threshold)\n",
    ").plot()\n",
    "\n",
    "display(\n",
    "    compute_ranking_metrics_at_k(\n",
    "        y_val.to_numpy(), anomaly_scores, [10, 15, 20, 25, 30, 40, 50, 100, 150, 200, 1000]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "- For threshold is > 0.16 the Isolation Forest Has Precsion of 30% and Recall of 30%\n",
    "- This is Much more deleberate than random Choice\n",
    "- This will help us to reduce the False Positives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Local Outlier Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third Party Imports\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "\n",
    "# Create the Isolation forest pipeline\n",
    "lof_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", RobustScaler()),\n",
    "        (\"lof\", LocalOutlierFactor(n_neighbors=200, novelty=True)),\n",
    "    ]\n",
    ")\n",
    "# Train the Isolation Forest on Non-Fradulant Train data\n",
    "lof_pipeline.fit(X_train_class_0)\n",
    "# Now get scores on the full training data\n",
    "anomaly_scores = -lof_pipeline.decision_function(X_val)\n",
    "\n",
    "# Plot the Anomaly Scores\n",
    "plot_anomaly_score(anomaly_scores).show()\n",
    "\n",
    "# Plot the PR Curve\n",
    "_, lof_threshold = plot_precision_recall_curve(y_val.to_numpy(), anomaly_scores)\n",
    "_.show()\n",
    "\n",
    "# Plot the ROC curve\n",
    "plot_roc_curve(y_val.to_numpy(), anomaly_scores).show()\n",
    "\n",
    "# Compute Average precision\n",
    "print(f\"Average Precision: {average_precision_score(y_val, anomaly_scores):.2%}\")\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_true=y_val, y_score=anomaly_scores):.2%}\")\n",
    "print(f\"LOF Threshold: {lof_threshold}\")\n",
    "\n",
    "# Display the confusion matrix\n",
    "_ = ConfusionMatrixDisplay(\n",
    "    confusion_matrix(y_true=y_val, y_pred=anomaly_scores > lof_threshold)\n",
    ").plot()\n",
    "\n",
    "display(\n",
    "    compute_ranking_metrics_at_k(\n",
    "        y_val.to_numpy(), anomaly_scores, [10, 15, 20, 25, 30, 40, 50, 100, 150, 200]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# **Autoencoder with ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third Party improts\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Internal Imports\n",
    "from fraud_detection.config.unsupervised import AutoEncoderConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Configuration File\n",
    "cfg = AutoEncoderConfig(**OmegaConf.load(\"../config/autoencoder/autoencoder.yaml\"))\n",
    "\n",
    "# Display data\n",
    "cfg.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "# Third Party Imports\n",
    "import jax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "class Params(NamedTuple):\n",
    "    # Layer 1\n",
    "    w1: jax.Array\n",
    "    b1: jax.Array\n",
    "    # Layer 2\n",
    "    w2: jax.Array\n",
    "    b2: jax.Array\n",
    "    # Layer 3\n",
    "    w3: jax.Array\n",
    "    b3: jax.Array\n",
    "    # Layer 4\n",
    "    w4: jax.Array\n",
    "    b4: jax.Array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "We are going for a simple Symmetric Architecture for the AutoEncoder Input -> Hidden1 -> Bottleneck -> Hidden2 -> Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(cfg) -> None:\n",
    "    # Set the Jax key\n",
    "    key = jax.random.key(cfg.seed)\n",
    "    # Number of keys\n",
    "    num_keys = 2 * (len(cfg.model.model_dump()) - 1)\n",
    "    # Split the key into 6 parts\n",
    "    (w1_key, b1_key, w2_key, b2_key, w3_key, b3_key, w4_key, b4_key) = jax.random.split(\n",
    "        key, num_keys\n",
    "    )\n",
    "\n",
    "    # Form weights\n",
    "    # Layer 1\n",
    "    w1 = jax.random.normal(\n",
    "        w1_key, (cfg.model.input_size, cfg.model.hidden1_size)\n",
    "    ) * jnp.sqrt(2 / cfg.model.input_size)\n",
    "    b1 = jax.random.normal(b1_key, (cfg.model.hidden1_size,))\n",
    "    # Layer 2\n",
    "    w2 = jax.random.normal(\n",
    "        w2_key, (cfg.model.hidden1_size, cfg.model.bottleneck_size)\n",
    "    ) * jnp.sqrt(2 / cfg.model.input_size)\n",
    "    b2 = jax.random.normal(b2_key, (cfg.model.bottleneck_size,))\n",
    "    # Layer 3\n",
    "    w3 = jax.random.normal(\n",
    "        w3_key, (cfg.model.bottleneck_size, cfg.model.hidden2_size)\n",
    "    ) * jnp.sqrt(2 / cfg.model.input_size)\n",
    "    b3 = jax.random.normal(b3_key, (cfg.model.hidden2_size,))\n",
    "    # Layer 4\n",
    "    w4 = jax.random.normal(\n",
    "        w4_key, (cfg.model.hidden2_size, cfg.model.output_size)\n",
    "    ) * jnp.sqrt(2 / cfg.model.input_size)\n",
    "    b4 = jax.random.normal(b4_key, (cfg.model.output_size,))\n",
    "    # Return the weights\n",
    "    return Params(w1=w1, b1=b1, w2=w2, b2=b2, w3=w3, b3=b3, w4=w4, b4=b4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def encoder_predict(params: Params, inputs: jax.Array) -> jax.Array:\n",
    "    # Start Computation\n",
    "    ## Layer 1\n",
    "    layer_1_out = jax.nn.relu(jnp.einsum(\"ih,hj->ij\", inputs, params.w1) + params.b1)\n",
    "    ## Layer 2\n",
    "    layer_2_out = jax.nn.relu(\n",
    "        jnp.einsum(\"ih,hj->ij\", layer_1_out, params.w2) + params.b2\n",
    "    )\n",
    "    return layer_2_out\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def decoder_predict(params: Params, inputs: jax.Array) -> jax.Array:\n",
    "    # Start Computation\n",
    "    ## Layer 1\n",
    "    layer_3_out = jax.nn.relu(jnp.einsum(\"ih,hj->ij\", inputs, params.w3) + params.b3)\n",
    "    ## Layer 2\n",
    "    layer_4_out = jnp.einsum(\"ih,hj->ij\", layer_3_out, params.w4) + params.b4\n",
    "    # Return the output\n",
    "    return layer_4_out\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def decision_function(params: Params, inputs: jax.Array) -> jax.Array:\n",
    "    # Encode Incoming data\n",
    "    encodes = encoder_predict(params=params, inputs=inputs)\n",
    "    # Decode the data\n",
    "    decodes = decoder_predict(params=params, inputs=encodes)\n",
    "    # Compute Reconstruction Error\n",
    "    return jnp.mean(jnp.square(inputs - decodes), axis=-1)\n",
    "\n",
    "\n",
    "def reconstruction_error(inputs: jax.Array, decodes: jax.Array) -> jax.Array:\n",
    "    ## Compute Reconstruction Error\n",
    "    return jnp.mean(jnp.absolute(inputs - decodes))\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def forward_pass(params: Params, inputs: jax.Array) -> jax.Array:\n",
    "    # Encode Incoming data\n",
    "    encodes = encoder_predict(params=params, inputs=inputs)\n",
    "    # Decode the data\n",
    "    decodes = decoder_predict(params=params, inputs=encodes)\n",
    "    # Compute Reconstruction Error\n",
    "    return reconstruction_error(inputs=inputs, decodes=decodes)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def value_and_grad(params: Params, inputs: jax.Array) -> tuple[jax.Array, jax.Array]:\n",
    "    # Use jax to compute value and gradient\n",
    "    return jax.value_and_grad(forward_pass)(params, inputs=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchState(NamedTuple):\n",
    "    \"\"\"\n",
    "    Immutable container for the iterator's state.\n",
    "    Passed in and out of the pure function.\n",
    "    \"\"\"\n",
    "\n",
    "    key: jax.random.PRNGKey\n",
    "    idx: int\n",
    "    indices: jax.Array\n",
    "\n",
    "\n",
    "class TrainState(NamedTuple):\n",
    "    params: jax.Array\n",
    "    opt_state: optax.OptState\n",
    "    batch_state: BatchState\n",
    "\n",
    "\n",
    "def init_batch_state(\n",
    "    key: jax.random.PRNGKey, data_size: int, init: bool = True\n",
    ") -> BatchState:\n",
    "    \"\"\"\n",
    "    Called once before training starts to set up the first epoch.\n",
    "    \"\"\"\n",
    "    # Function when init is true\n",
    "    shuffled_indices = jax.random.permutation(key, data_size)\n",
    "    return BatchState(key=key, idx=0, indices=shuffled_indices)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def get_batch(\n",
    "    batch_state: BatchState, X: jax.Array, batch_array: jax.Array\n",
    ") -> [jax.Array, BatchState]:\n",
    "    # Get the indices for the current batch\n",
    "    next_idx = batch_state.idx + batch_array.shape[0]\n",
    "    # is the next-id out of bounds?\n",
    "    epoch_ended = next_idx >= X.shape[0]\n",
    "\n",
    "    # Get Batch Indices and start over again if the index is out of bounds\n",
    "    batch_indices = jnp.take(\n",
    "        batch_state.indices, (batch_state.idx + batch_array) % X.shape[0], mode=\"clip\"\n",
    "    )\n",
    "    # Take samples from the dataset\n",
    "    X_batch = jnp.take(X, batch_indices, axis=0)\n",
    "\n",
    "    def _epoch_continues(_):\n",
    "        return BatchState(\n",
    "            key=batch_state.key, idx=next_idx, indices=batch_state.indices\n",
    "        )\n",
    "\n",
    "    def _epoch_ended(_):\n",
    "        # Splt the keys\n",
    "        new_key, shuffle_key = jax.random.split(batch_state.key)\n",
    "        # Shuffle the indices\n",
    "        new_shuffled_indices = jax.random.permutation(shuffle_key, X.shape[0])\n",
    "        # Return new state\n",
    "        return BatchState(key=new_key, idx=0, indices=new_shuffled_indices)\n",
    "\n",
    "    # The condition for the batch\n",
    "    return X_batch, jax.lax.cond(epoch_ended, _epoch_ended, _epoch_continues, None)\n",
    "\n",
    "\n",
    "def get_train_step(\n",
    "    train_state: TrainState,\n",
    "    X: jax.Array,\n",
    "    batch_array: jax.Array,\n",
    "    optimizer: optax.GradientTransformation,\n",
    "):\n",
    "    \"\"\"Factory Pattern Solution to Optimizer\"\"\"\n",
    "\n",
    "    def _train_step(train_state: TrainState, _: Any) -> tuple[TrainState, jax.Array]:\n",
    "        # Get batches\n",
    "        X_batch, batch_state = get_batch(\n",
    "            batch_state=train_state.batch_state,\n",
    "            X=X,\n",
    "            batch_array=batch_array,\n",
    "        )\n",
    "        # Get value and gradients\n",
    "        train_loss, gradients = value_and_grad(train_state.params, X_batch)\n",
    "        # Update the parameters\n",
    "        updates, opt_state = optimizer.update(\n",
    "            gradients, train_state.opt_state, train_state.params\n",
    "        )\n",
    "        params = optax.apply_updates(train_state.params, updates)\n",
    "        # Updated the train state\n",
    "        train_state = TrainState(\n",
    "            params=params, opt_state=opt_state, batch_state=batch_state\n",
    "        )\n",
    "        return train_state, train_loss\n",
    "\n",
    "    return jax.jit(_train_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Parameters\n",
    "params = init_params(cfg)\n",
    "\n",
    "# lets scale the data first\n",
    "scaler = RobustScaler()\n",
    "# Scale the data\n",
    "X_train_class_0_sc = jnp.array(\n",
    "    scaler.fit_transform(X_train_class_0.to_numpy()), dtype=jnp.float32\n",
    ")\n",
    "X_val_class_0_sc = jnp.array(\n",
    "    scaler.transform(X_val.filter(y_val == 0).to_numpy()), dtype=jnp.float32\n",
    ")\n",
    "\n",
    "# Set up Learning Scheduler\n",
    "schedulers = optax.schedules.warmup_cosine_decay_schedule(\n",
    "    init_value=cfg.training.schedulers.init_value,\n",
    "    peak_value=cfg.training.schedulers.peak_value,\n",
    "    warmup_steps=cfg.training.schedulers.warmup_steps,\n",
    "    decay_steps=cfg.training.schedulers.decay_steps,\n",
    "    end_value=cfg.training.schedulers.end_value,\n",
    "    exponent=cfg.training.schedulers.exponent,\n",
    ")\n",
    "\n",
    "# Set optimizer\n",
    "optimizer = optax.adamw(learning_rate=schedulers)\n",
    "# Intitialize optimizer state\n",
    "opt_state = optimizer.init(params=params)\n",
    "\n",
    "\n",
    "# Batching key\n",
    "key = jax.random.key(cfg.training.shuffle_seed)\n",
    "# Get initial State\n",
    "batch_state = init_batch_state(key, X_train_class_0_sc.shape[0])\n",
    "# batch Size\n",
    "batch_size = cfg.training.batch_size\n",
    "batch_array = jnp.arange(0, batch_size)\n",
    "# Get Number of batches\n",
    "epochs = cfg.training.epochs\n",
    "\n",
    "# Print Coverage\n",
    "print(\"Coverage:\", batch_size * epochs // X_train_class_0_sc.shape[0])\n",
    "print(\"Batch Coverage:\", X_train_class_0_sc.shape[0] // batch_size)\n",
    "\n",
    "# Initialize the train state\n",
    "train_state = TrainState(\n",
    "    params=params,\n",
    "    opt_state=opt_state,\n",
    "    batch_state=batch_state,\n",
    ")\n",
    "\n",
    "# Get the train step func\n",
    "train_step = get_train_step(train_state, X_train_class_0_sc, batch_array, optimizer)\n",
    "\n",
    "# Run loop\n",
    "train_state, train_loss = jax.lax.scan(\n",
    "    train_step, init=train_state, xs=None, length=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.arange(len(train_loss)),\n",
    "        y=np.array(train_loss),\n",
    "        hovertemplate=\"Approximate Batch: %{x}<br>Rolling Loss: %{y}\",\n",
    "        name=\"Rolling MAE Loss\",\n",
    "        showlegend=False,\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=dict(text=\"<b>Training Loss</b>\", x=0.5, font=dict(size=30)),\n",
    "    xaxis_title=\"<b>Batch</b>\",\n",
    "    yaxis_title=\"<b>Loss</b>\",\n",
    "    template=\"plotly_dark\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss is smoothed for better visualization over the approximately the full data tlength\n",
    "train_loss_smoothed = jnp.convolve(\n",
    "    train_loss,\n",
    "    jnp.full(\n",
    "        X_train_class_0_sc.shape[0] // batch_size,\n",
    "        fill_value=(1 / (X_train_class_0_sc.shape[0] // batch_size)),\n",
    "    ),\n",
    "    mode=\"valid\",\n",
    ")\n",
    "\n",
    "\n",
    "# Plotting\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.arange(len(train_loss_smoothed)),\n",
    "        y=np.array(train_loss_smoothed),\n",
    "        hovertemplate=\"Approximate Batch: %{x}<br>Rolling Loss: %{y}\",\n",
    "        name=\"Rolling MAE Loss\",\n",
    "        showlegend=False,\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=dict(text=\"<b>Training Loss</b>\", x=0.5, font=dict(size=30)),\n",
    "    xaxis_title=\"<b>Batch</b>\",\n",
    "    yaxis_title=\"<b>Loss</b>\",\n",
    "    template=\"plotly_dark\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Incoming data\n",
    "encodes = encoder_predict(params=train_state.params, inputs=X_val_class_0_sc)\n",
    "# Decode the data\n",
    "decodes = decoder_predict(params=train_state.params, inputs=encodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 100\n",
    "for val, pred in zip(X_val_class_0_sc[idx], decodes[idx]):\n",
    "    print(f\"{val:.2f} -> {pred:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get scores on the full training data\n",
    "anomaly_scores = np.array(\n",
    "    decision_function(\n",
    "        params=train_state.params,\n",
    "        inputs=jnp.array(scaler.transform(X_val.to_numpy()), dtype=jnp.float32),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot the Anomaly Scores\n",
    "plot_anomaly_score(anomaly_scores).show()\n",
    "\n",
    "# Plot the PR Curve\n",
    "_, ae_threshold = plot_precision_recall_curve(y_val.to_numpy(), anomaly_scores)\n",
    "_.show()\n",
    "\n",
    "# Plot the ROC curve\n",
    "plot_roc_curve(y_val.to_numpy(), anomaly_scores).show()\n",
    "\n",
    "# Compute Average precision\n",
    "print(f\"Average Precision: {average_precision_score(y_val, anomaly_scores):.2%}\")\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_true=y_val, y_score=anomaly_scores):.2%}\")\n",
    "print(f\"AE Forest Threshold: {ae_threshold}\")\n",
    "\n",
    "# Display the confusion matrix\n",
    "_ = ConfusionMatrixDisplay(\n",
    "    confusion_matrix(y_true=y_val, y_pred=(anomaly_scores > ae_threshold))\n",
    ").plot()\n",
    "\n",
    "display(\n",
    "    compute_ranking_metrics_at_k(\n",
    "        y_val.to_numpy(), anomaly_scores, [10, 50, 100, 200, 300, 300, 1000]\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
