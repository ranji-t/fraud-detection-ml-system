{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# **Fraud Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third Party Imports\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "# Internal Imports\n",
    "from fraud_detection.metrics import precision_at_K, recall_at_K, lift_at_K\n",
    "from fraud_detection.utils.plotting import (\n",
    "    plot_anomaly_score,\n",
    "    plot_precision_recall_curve,\n",
    "    plot_roc_curve,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## *Read data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = pl.read_csv(\n",
    "    r\"../data/input/creditcard.csv\",\n",
    "    ignore_errors=False,\n",
    "    infer_schema_length=1000_000,\n",
    ")\n",
    "\n",
    "# Display data\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# **Feature/Target & Train/Test/Validation Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.select(pl.exclude(\"Time\", \"Class\"))\n",
    "y = df.select(\"Class\").to_series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time based split\n",
    "## Train Data set\n",
    "X_train = X[: 2 * (X.height // 4)]\n",
    "y_train = y[: 2 * (y.len() // 4)]\n",
    "\n",
    "## Validation Data set\n",
    "X_val = X[2 * (X.height // 4) : 3 * (X.height // 4)]\n",
    "y_val = y[2 * (y.len() // 4) : 3 * (y.len() // 4)]\n",
    "\n",
    "# Test Data set\n",
    "X_test = X[3 * (X.height // 4) :]\n",
    "y_test = y[3 * (y.len() // 4) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display class distribution in training set\n",
    "display(\n",
    "    y_train.value_counts()\n",
    "    .sort(by=\"Class\")\n",
    "    .with_columns(\n",
    "        (pl.col(\"count\") * 100 / pl.col(\"count\").sum()).round(4).alias(\"percentage\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display class distribution in valid set\n",
    "display(\n",
    "    y_val.value_counts()\n",
    "    .sort(by=\"Class\")\n",
    "    .with_columns(\n",
    "        (pl.col(\"count\") * 100 / pl.col(\"count\").sum()).round(4).alias(\"percentage\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display class distribution in test set\n",
    "display(\n",
    "    y_test.value_counts()\n",
    "    .sort(by=\"Class\")\n",
    "    .with_columns(\n",
    "        (pl.col(\"count\") * 100 / pl.col(\"count\").sum()).round(4).alias(\"percentage\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# **Outlier Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## *Approach A â€” Isolation-Based Anomaly Detection*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only filter class 0 from the training data\n",
    "X_train_class_0 = X_train.filter(y_train == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ranking_metrics_at_k(\n",
    "    y_true: np.ndarray, y_pred_proba: np.ndarray, K_list: list[int]\n",
    ") -> pl.DataFrame:\n",
    "    # Has to be numpy array\n",
    "    if not isinstance(y_true, np.ndarray):\n",
    "        raise ValueError(\"y_true must be a numpy array\")\n",
    "    if not isinstance(y_pred_proba, np.ndarray):\n",
    "        raise ValueError(\"y_pred_proba must be a numpy array\")\n",
    "    # if not i Dimension array\n",
    "    if y_true.ndim != 1:\n",
    "        raise ValueError(\"y_true must be a 1D array\")\n",
    "    if y_pred_proba.ndim != 1:\n",
    "        raise ValueError(\"y_pred_proba must be a 1D array\")\n",
    "    # Constrains on K\n",
    "    if min(K_list) < 1:\n",
    "        raise ValueError(\"K must be greater than 0\")\n",
    "    if not all([isinstance(k, int) for k in K_list]):\n",
    "        raise ValueError(\"K must be an integer\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"K\": K_list,\n",
    "        \"Precision@K\": [precision_at_K(y_true, y_pred_proba, K) for K in K_list],\n",
    "        \"Recall@K\": [recall_at_K(y_true, y_pred_proba, K) for K in K_list],\n",
    "        \"Lift@K\": [lift_at_K(y_true, y_pred_proba, K) for K in K_list],\n",
    "    }\n",
    "    return pl.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third Party Imports\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Create the Isolation forest pipeline\n",
    "if_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", RobustScaler()),\n",
    "        (\"iso_forest\", IsolationForest(n_estimators=1_000, random_state=42, n_jobs=5)),\n",
    "    ]\n",
    ")\n",
    "# Train the Isolation Forest on Non-Fradulant Train data\n",
    "if_pipeline.fit(X_train_class_0)\n",
    "# Now get scores on the full training data\n",
    "anomaly_scores = -if_pipeline.decision_function(X_val)\n",
    "\n",
    "# Plot the Anomaly Scores\n",
    "plot_anomaly_score(anomaly_scores).show()\n",
    "\n",
    "# Plot the PR Curve\n",
    "_, if_threshold = plot_precision_recall_curve(y_val.to_numpy(), anomaly_scores)\n",
    "_.show()\n",
    "\n",
    "# Plot the ROC curve\n",
    "plot_roc_curve(y_val.to_numpy(), anomaly_scores).show()\n",
    "\n",
    "# Compute Average precision\n",
    "print(f\"Average Precision: {average_precision_score(y_val, anomaly_scores):.2%}\")\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_true=y_val, y_score=anomaly_scores):.2%}\")\n",
    "print(f\"Isolation Forest Threshold: {if_threshold}\")\n",
    "\n",
    "# Display the confusion matrix\n",
    "_ = ConfusionMatrixDisplay(\n",
    "    confusion_matrix(y_true=y_val, y_pred=anomaly_scores > if_threshold)\n",
    ").plot()\n",
    "\n",
    "display(\n",
    "    compute_ranking_metrics_at_k(\n",
    "        y_val.to_numpy(), anomaly_scores, [10, 15, 20, 25, 30, 40, 50, 100, 150, 200]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "- For threshold is > 0.16 the Isolation Forest Has Precsion of 30% and Recall of 30%\n",
    "- This is Much more deleberate than random Choice\n",
    "- This will help us to reduce the False Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third Party Imports\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "\n",
    "# Create the Isolation forest pipeline\n",
    "lof_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", RobustScaler()),\n",
    "        (\"lof\", LocalOutlierFactor(n_neighbors=200, novelty=True)),\n",
    "    ]\n",
    ")\n",
    "# Train the Isolation Forest on Non-Fradulant Train data\n",
    "lof_pipeline.fit(X_train_class_0)\n",
    "# Now get scores on the full training data\n",
    "anomaly_scores = -lof_pipeline.decision_function(X_val)\n",
    "\n",
    "# Plot the Anomaly Scores\n",
    "plot_anomaly_score(anomaly_scores).show()\n",
    "\n",
    "# Plot the PR Curve\n",
    "_, lof_threshold = plot_precision_recall_curve(y_val.to_numpy(), anomaly_scores)\n",
    "_.show()\n",
    "\n",
    "# Plot the ROC curve\n",
    "plot_roc_curve(y_val.to_numpy(), anomaly_scores).show()\n",
    "\n",
    "# Compute Average precision\n",
    "print(f\"Average Precision: {average_precision_score(y_val, anomaly_scores):.2%}\")\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_true=y_val, y_score=anomaly_scores):.2%}\")\n",
    "print(f\"Isolation Forest Threshold: {lof_threshold}\")\n",
    "\n",
    "# Display the confusion matrix\n",
    "_ = ConfusionMatrixDisplay(\n",
    "    confusion_matrix(y_true=y_val, y_pred=anomaly_scores > if_threshold)\n",
    ").plot()\n",
    "\n",
    "display(\n",
    "    compute_ranking_metrics_at_k(\n",
    "        y_val.to_numpy(), anomaly_scores, [10, 15, 20, 25, 30, 40, 50, 100, 150, 200]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# **Autoencoder with ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third Party improts\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Internal Imports\n",
    "from fraud_detection.config.unsupervised import AutoEncoderConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Configuration File\n",
    "cfg = AutoEncoderConfig(**OmegaConf.load(\"../config/autoencoder/autoencoder.yaml\"))\n",
    "\n",
    "# Display data\n",
    "cfg.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "# Third Party Imports\n",
    "import jax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "class Params(NamedTuple):\n",
    "    # Layer 1\n",
    "    w1: jax.Array\n",
    "    b1: jax.Array\n",
    "    # Layer 2\n",
    "    w2: jax.Array\n",
    "    b2: jax.Array\n",
    "    # Layer 3\n",
    "    w3: jax.Array\n",
    "    b3: jax.Array\n",
    "    # Layer 4\n",
    "    w4: jax.Array\n",
    "    b4: jax.Array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "We are going for a simple Symmetric Architecture for the AutoEncoder Input -> Hidden1 -> Bottleneck -> Hidden2 -> Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(cfg) -> None:\n",
    "    # Set the Jax key\n",
    "    key = jax.random.key(cfg.seed)\n",
    "    # Number of keys\n",
    "    num_keys = 2 * (len(cfg.model.model_dump()) - 1)\n",
    "    # Split the key into 6 parts\n",
    "    (w1_key, b1_key, w2_key, b2_key, w3_key, b3_key, w4_key, b4_key) = jax.random.split(\n",
    "        key, num_keys\n",
    "    )\n",
    "\n",
    "    # Form weights\n",
    "    # Layer 1\n",
    "    w1 = jax.random.normal(\n",
    "        w1_key, (cfg.model.input_size, cfg.model.hidden1_size)\n",
    "    ) * jnp.sqrt(2 / cfg.model.input_size)\n",
    "    b1 = jax.random.normal(b1_key, (cfg.model.hidden1_size,))\n",
    "    # Layer 2\n",
    "    w2 = jax.random.normal(\n",
    "        w2_key, (cfg.model.hidden1_size, cfg.model.bottleneck_size)\n",
    "    ) * jnp.sqrt(2 / cfg.model.input_size)\n",
    "    b2 = jax.random.normal(b2_key, (cfg.model.bottleneck_size,))\n",
    "    # Layer 3\n",
    "    w3 = jax.random.normal(\n",
    "        w3_key, (cfg.model.bottleneck_size, cfg.model.hidden2_size)\n",
    "    ) * jnp.sqrt(2 / cfg.model.input_size)\n",
    "    b3 = jax.random.normal(b3_key, (cfg.model.hidden2_size,))\n",
    "    # Layer 4\n",
    "    w4 = jax.random.normal(\n",
    "        w4_key, (cfg.model.hidden2_size, cfg.model.output_size)\n",
    "    ) * jnp.sqrt(2 / cfg.model.input_size)\n",
    "    b4 = jax.random.normal(b4_key, (cfg.model.output_size,))\n",
    "    # Return the weights\n",
    "    return Params(w1=w1, b1=b1, w2=w2, b2=b2, w3=w3, b3=b3, w4=w4, b4=b4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def encoder_predict(params: Params, inputs: jax.Array) -> jax.Array:\n",
    "    # Start Computation\n",
    "    ## Layer 1\n",
    "    layer_1_out = jax.nn.relu(jnp.einsum(\"ih,hj->ij\", inputs, params.w1) + params.b1)\n",
    "    ## Layer 2\n",
    "    layer_2_out = jax.nn.relu(\n",
    "        jnp.einsum(\"ih,hj->ij\", layer_1_out, params.w2) + params.b2\n",
    "    )\n",
    "    return layer_2_out\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def decoder_predict(params: Params, inputs: jax.Array) -> jax.Array:\n",
    "    # Start Computation\n",
    "    ## Layer 1\n",
    "    layer_3_out = jax.nn.relu(jnp.einsum(\"ih,hj->ij\", inputs, params.w3) + params.b3)\n",
    "    ## Layer 2\n",
    "    layer_4_out = jnp.einsum(\"ih,hj->ij\", layer_3_out, params.w4) + params.b4\n",
    "    # Return the output\n",
    "    return layer_4_out\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def decision_function(params: Params, inputs: jax.Array) -> jax.Array:\n",
    "    # Encode Incoming data\n",
    "    encodes = encoder_predict(params=params, inputs=inputs)\n",
    "    # Decode the data\n",
    "    decodes = decoder_predict(params=params, inputs=encodes)\n",
    "    # Compute Reconstruction Error\n",
    "    return jnp.mean(jnp.square(inputs - decodes), axis=-1)\n",
    "\n",
    "\n",
    "def reconstruction_error(inputs: jax.Array, decodes: jax.Array) -> jax.Array:\n",
    "    ## Compute Reconstruction Error\n",
    "    return jnp.mean(jnp.square(inputs - decodes))\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def forward_pass(params: Params, inputs: jax.Array) -> jax.Array:\n",
    "    # Encode Incoming data\n",
    "    encodes = encoder_predict(params=params, inputs=inputs)\n",
    "    # Decode the data\n",
    "    decodes = decoder_predict(params=params, inputs=encodes)\n",
    "    # Compute Reconstruction Error\n",
    "    return reconstruction_error(inputs=inputs, decodes=decodes)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def value_and_grad(params: Params, inputs: jax.Array) -> tuple[jax.Array, jax.Array]:\n",
    "    # Use jax to compute value and gradient\n",
    "    return jax.value_and_grad(forward_pass)(params, inputs=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Parameters\n",
    "params = init_params(cfg)\n",
    "\n",
    "# lets scale the data first\n",
    "scaler = RobustScaler()\n",
    "# Scale the data\n",
    "X_train_class_0_sc = jnp.array(\n",
    "    scaler.fit_transform(X_train_class_0.to_numpy()), dtype=jnp.float32\n",
    ")\n",
    "X_val_class_0_sc = jnp.array(\n",
    "    scaler.transform(X_val.filter(y_val == 0).to_numpy()), dtype=jnp.float32\n",
    ")\n",
    "\n",
    "# Set the Learning rate scheduler\n",
    "lr_scheduler = optax.schedules.join_schedules(\n",
    "    [\n",
    "        optax.schedules.linear_schedule(\n",
    "            init_value=cfg.training.schedulers.linear_schedule.init_value,\n",
    "            end_value=cfg.training.schedulers.linear_schedule.end_value,\n",
    "            transition_steps=cfg.training.schedulers.linear_schedule.transition_steps,\n",
    "        ),\n",
    "        optax.schedules.cosine_decay_schedule(\n",
    "            init_value=cfg.training.schedulers.cosine_decay_schedule.init_value,\n",
    "            decay_steps=cfg.training.schedulers.cosine_decay_schedule.decay_steps,\n",
    "            alpha=cfg.training.schedulers.cosine_decay_schedule.alpha,\n",
    "        ),\n",
    "    ],\n",
    "    boundaries=cfg.training.schedulers.boundaries,\n",
    ")\n",
    "# Intialize the optimizer\n",
    "optimizer = optax.adam(lr_scheduler)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# Hitory\n",
    "history = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "# Trial of loop\n",
    "for i in range(cfg.training.epochs):\n",
    "    # Get value and gradients\n",
    "    train_loss, gradients = value_and_grad(params, X_train_class_0_sc)\n",
    "    # The Validation loss\n",
    "    val_loss = forward_pass(params, X_val_class_0_sc)\n",
    "    # Update the parameters\n",
    "    updates, opt_state = optimizer.update(gradients, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    # Store the loss\n",
    "    history[\"train_loss\"].append(float(train_loss))\n",
    "    history[\"val_loss\"].append(float(val_loss))\n",
    "\n",
    "\n",
    "# Set Figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add Line\n",
    "fig.add_trace(go.Scatter(y=history[\"train_loss\"], name=\"Train Loss\"))\n",
    "fig.add_trace(go.Scatter(y=history[\"val_loss\"], name=\"Validation Loss\"))\n",
    "\n",
    "# Update Layout\n",
    "fig.update_layout(\n",
    "    title=dict(text=\"Training and Validation Loss\", x=0.5, font=dict(size=30)),\n",
    "    xaxis_title=\"Epoch\",\n",
    "    yaxis_title=\"Loss\",\n",
    "    template=\"plotly_dark\",\n",
    ")\n",
    "\n",
    "# Show Figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchState(NamedTuple):\n",
    "    key: jax.random.PRNGKey\n",
    "    idx: int\n",
    "    indices: jax.Array\n",
    "\n",
    "\n",
    "def init_batch_state(key: jax.random.PRNGKey, data_size: int) -> BatchState:\n",
    "    # 1. Get Suffeled indices\n",
    "    shuffled_indices = jax.random.permutation(key, data_size)\n",
    "    # 2. Return the batch state\n",
    "    return BatchState(key=key, idx=0, indices=shuffled_indices)\n",
    "\n",
    "\n",
    "def get_batch(batch_state: BatchState, X: jax.Array, batch_size: int) -> BatchState:\n",
    "    # Get Data Size\n",
    "    data_size = X.shape[0]\n",
    "    # Get the indices for the current batch\n",
    "    next_idx = batch_state.idx + batch_size\n",
    "    # Check if the epoch has ended\n",
    "    epoch_ended = next_idx >= data_size\n",
    "\n",
    "    # 1. Get the indices for the current batch\n",
    "    batch_indices = jnp.take(batch_state.indices, jnp.arange(batch_state.idx, next_idx))\n",
    "    X_batch = jnp.take(X, batch_indices, axis=0)\n",
    "\n",
    "    # 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching key\n",
    "key = jax.random.key(101)\n",
    "# Get initial State\n",
    "batch = init_batch_state(key, X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get scores on the full training data\n",
    "anomaly_scores = np.array(decision_function(params, X_val.to_jax()))\n",
    "\n",
    "# Plot the Anomaly Scores\n",
    "plot_anomaly_score(anomaly_scores).show()\n",
    "\n",
    "# Plot the PR Curve\n",
    "_, lof_threshold = plot_precision_recall_curve(y_val.to_numpy(), anomaly_scores)\n",
    "_.show()\n",
    "\n",
    "# Plot the ROC curve\n",
    "plot_roc_curve(y_val.to_numpy(), anomaly_scores).show()\n",
    "\n",
    "# Compute Average precision\n",
    "print(f\"Average Precision: {average_precision_score(y_val, anomaly_scores):.2%}\")\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_true=y_val, y_score=anomaly_scores):.2%}\")\n",
    "print(f\"Isolation Forest Threshold: {lof_threshold}\")\n",
    "\n",
    "# Display the confusion matrix\n",
    "_ = ConfusionMatrixDisplay(\n",
    "    confusion_matrix(y_true=y_val, y_pred=anomaly_scores > if_threshold)\n",
    ").plot()\n",
    "\n",
    "display(\n",
    "    compute_ranking_metrics_at_k(\n",
    "        y_val.to_numpy(), anomaly_scores, [10, 15, 20, 25, 30, 40, 50, 100, 150, 200]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_scores > if_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "if_threshold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
