{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# **Fraud Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "from typing import Callable\n",
    "from functools import partial\n",
    "from pprint import pprint\n",
    "\n",
    "# Third Party Imports\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_predict,\n",
    "    cross_val_score,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    average_precision_score,\n",
    "    log_loss,\n",
    "    brier_score_loss,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    balanced_accuracy_score,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from optuna.importance import get_param_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## *Read data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = pl.read_csv(\n",
    "    r\"D:\\Codebase\\fraud-detection\\data\\input\\creditcard.csv\",\n",
    "    ignore_errors=False,\n",
    "    infer_schema_length=1000_000,\n",
    ")\n",
    "\n",
    "# Display data\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# **Data Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## *Data Split*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into X and y\n",
    "x = df.select(pl.exclude(\"Class\")).to_pandas()\n",
    "y = df.select(\"Class\").to_series().to_pandas()\n",
    "\n",
    "# Set type of the splits\n",
    "X_train: pd.DataFrame\n",
    "y_train: pd.Series\n",
    "X_test: pd.DataFrame\n",
    "y_test: pd.Series\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# The class distribution in training set\n",
    "display(y_train.value_counts(normalize=False))\n",
    "display(y_train.value_counts(normalize=True).round(4) * 100)\n",
    "\n",
    "# The Target Mapping\n",
    "target_mapping = {0: \"Legitimate\", 1: \"Fraudulent\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# **Modelling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def better_classification_report(\n",
    "    model,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    ") -> np.ndarray:\n",
    "    # Set up Cross Validation Policy\n",
    "    cv_policy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    y_train_proba = cross_val_predict(\n",
    "        model, X_train, y_train, cv=cv_policy, method=\"predict_proba\", verbose=0\n",
    "    )\n",
    "    # Get Predictions using Cross Validation\n",
    "    y_train_pred = np.where(y_train_proba[:, 1] > 0.5, 1, 0)\n",
    "\n",
    "    # Show Basic Metrics\n",
    "    print(f\"Accuracy         : {accuracy_score(y_train, y_train_pred):.2%}\")\n",
    "    print(f\"Balanced Accuracy: {balanced_accuracy_score(y_train, y_train_pred):.2%}\")\n",
    "    print(f\"F1 Score         : {f1_score(y_train, y_train_pred):.2%}\")\n",
    "    print(f\"Precision        : {precision_score(y_train, y_train_pred):.2%}\")\n",
    "    print(f\"Recall           : {recall_score(y_train, y_train_pred):.2%}\")\n",
    "    print(f\"AP Score    : {average_precision_score(y_train, y_train_proba[:, 1]):.2%}\")\n",
    "    print(f\"Log Loss    : {log_loss(y_train, y_train_proba):.4f}\")\n",
    "    print(f\"Brier Score : {brier_score_loss(y_train, y_train_proba[:, 1]):.4f}\")\n",
    "\n",
    "    # Show Confusion Matrix\n",
    "    display(\n",
    "        pd.DataFrame(\n",
    "            confusion_matrix(y_train, y_train_pred),\n",
    "            columns=[\"Pred-Normal-Transaction\", \"Pred-Fraud-Transaction\"],\n",
    "            index=[\"Actual-Normal-Transaction\", \"Actual-Fraud-Transaction\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Show Classification Report\n",
    "    print(classification_report(y_train, y_train_pred, zero_division=0))\n",
    "\n",
    "    # Return nothing\n",
    "    return y_train_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Objective Function type\n",
    "MultiObjectiveFunction = Callable[[optuna.trial.Trial], tuple[float, float, float]]\n",
    "FullMultiObjectiveFunction = Callable[\n",
    "    [optuna.trial.Trial, pd.DataFrame, pd.Series], tuple[float, float, float]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_dt_multi_objective_func(\n",
    "    trial: optuna.trial.Trial, X_train: pd.DataFrame, y_train: pd.Series\n",
    ") -> tuple[float, float, float]:\n",
    "    # Parameter Dict for this trial\n",
    "    params_dict = dict(\n",
    "        ## Tree Policy\n",
    "        criterion=trial.suggest_categorical(\n",
    "            \"criterion\", [\"gini\", \"entropy\", \"log_loss\"]\n",
    "        ),\n",
    "        splitter=trial.suggest_categorical(\"splitter\", [\"random\", \"best\"]),\n",
    "        class_weight=trial.suggest_categorical(\"class_weight\", [\"balanced\", None]),\n",
    "        ## pre-pruning\n",
    "        max_depth=trial.suggest_int(\"max_depth\", 1, 500, log=True),\n",
    "        min_samples_split=trial.suggest_int(\"min_samples_split\", 2, 100, log=True),\n",
    "        min_samples_leaf=trial.suggest_int(\"min_samples_leaf\", 1, 100, log=True),\n",
    "        # post-pruning\n",
    "        ccp_alpha=trial.suggest_float(\"ccp_alpha\", 1e-12, 1.5, log=True),\n",
    "    )\n",
    "\n",
    "    # Set the paramters in the decision tree\n",
    "    dt = DecisionTreeClassifier(random_state=42)\n",
    "    dt = dt.set_params(**params_dict)\n",
    "\n",
    "    # Set Cross Validation Policy & Cross Val Score\n",
    "    cv_policy = StratifiedKFold(n_splits=5, shuffle=True, random_state=1729)\n",
    "\n",
    "    # Get Predictions using Cross Validation\n",
    "    y_pred_proba = cross_val_predict(\n",
    "        dt, X_train, y_train, cv=cv_policy, n_jobs=-1, method=\"predict_proba\"\n",
    "    )[:, 1]\n",
    "    y_pred = np.where(y_pred_proba >= 0.5, 1, 0)\n",
    "\n",
    "    # Calculate Precision Score & Recall Score\n",
    "    precision = precision_score(y_train, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_train, y_pred, zero_division=0)\n",
    "    ap_score = average_precision_score(y_train, y_pred_proba)\n",
    "\n",
    "    # Return the balanced accuracy score\n",
    "    return float(ap_score), float(precision), float(recall)\n",
    "\n",
    "\n",
    "# An objective fucniton for Random Forest Classifier\n",
    "def full_rf_multi_objective_func(\n",
    "    trial: optuna.trial.Trial, X_train: pd.DataFrame, y_train: pd.Series\n",
    ") -> tuple[float, float, float]:\n",
    "    # Parameter Dict for this trial\n",
    "    params_dict = dict(\n",
    "        ## Tree Policy\n",
    "        class_weight=trial.suggest_categorical(\n",
    "            \"class_weight\", [\"balanced\", \"balanced_subsample\", None]\n",
    "        ),\n",
    "        ## pre-pruning\n",
    "        min_samples_split=trial.suggest_int(\"min_samples_split\", 2, 100, log=True),\n",
    "        min_samples_leaf=trial.suggest_int(\"min_samples_leaf\", 1, 100, log=True),\n",
    "        max_leaf_nodes=trial.suggest_int(\"max_leaf_nodes\", 10, 1000, log=True),\n",
    "        # post-pruning\n",
    "        ccp_alpha=trial.suggest_float(\"ccp_alpha\", 1e-9, 2, log=True),\n",
    "    )\n",
    "\n",
    "    # Set the paramters in the decision tree\n",
    "    rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    rf = rf.set_params(**params_dict)\n",
    "\n",
    "    # Set Cross Validation Policy & Cross Val Score\n",
    "    cv_policy = StratifiedKFold(n_splits=5, shuffle=True, random_state=1729)\n",
    "\n",
    "    # Get Predictions using Cross Validation\n",
    "    y_pred_proba = cross_val_predict(\n",
    "        rf, X_train, y_train, cv=cv_policy, n_jobs=-1, method=\"predict_proba\"\n",
    "    )[:, 1]\n",
    "    y_pred = np.where(y_pred_proba >= 0.5, 1, 0)\n",
    "\n",
    "    # Calculate Precision Score & Recall Score\n",
    "    precision = precision_score(y_train, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_train, y_pred, zero_division=0)\n",
    "    ap_score = average_precision_score(y_train, y_pred_proba)\n",
    "\n",
    "    # Return the balanced accuracy score\n",
    "    return float(ap_score), float(precision), float(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## *1. Dummy Classifier* : Setting Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show better classification report for dummy clf\n",
    "better_classification_report(\n",
    "    DummyClassifier(strategy=\"stratified\", random_state=42), X_train, y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## *2. Decision Tree*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Classifier Tree\n",
    "dt = DecisionTreeClassifier(\n",
    "    # Tree Policy\n",
    "    criterion=\"gini\",\n",
    "    splitter=\"best\",\n",
    "    # Weights\n",
    "    class_weight=\"balanced\",\n",
    "    # The shape of the tree\n",
    "    ## Pre-pruning\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    ## Post-pruning\n",
    "    ccp_alpha=0.0,\n",
    "    # Random Seed\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "\n",
    "# Show better classification report\n",
    "y_train_proba = better_classification_report(dt, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database path\n",
    "db_path = \"sqlite:///D:/Codebase/fraud-detection/data/db/optuna-fraud-detection.db\"\n",
    "\n",
    "# Create study\n",
    "study_dt = optuna.create_study(\n",
    "    storage=db_path,  # storage=None,\n",
    "    study_name=\"MOO-Decision-Tree-V02\",\n",
    "    directions=[\"maximize\", \"maximize\", \"maximize\"],\n",
    "    load_if_exists=True,\n",
    ")\n",
    "\n",
    "# Create the Partial Functions\n",
    "dt_multi_objective_func: MultiObjectiveFunction = partial(\n",
    "    full_dt_multi_objective_func, X_train=X_train, y_train=y_train\n",
    ")\n",
    "\n",
    "# Start Optimization\n",
    "study_dt.optimize(\n",
    "    dt_multi_objective_func,\n",
    "    n_trials=1,\n",
    "    timeout=int(3 * 3600),\n",
    "    n_jobs=1,\n",
    "    gc_after_trial=True,\n",
    "    show_progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand the Importtant Parameters\n",
    "importances_dt_ap = get_param_importances(study_dt, target=lambda t: t.values[0])\n",
    "importances_dt_pr = get_param_importances(study_dt, target=lambda t: t.values[1])\n",
    "importances_dt_re = get_param_importances(study_dt, target=lambda t: t.values[2])\n",
    "\n",
    "\n",
    "# Get the best trials\n",
    "best_trials_dt = study_dt.best_trials\n",
    "\n",
    "# Create Importances DataFrame\n",
    "importances_dt_df = pd.concat(\n",
    "    [\n",
    "        (pd.Series(importances_dt_ap, name=\"AP-Score\") * 100),\n",
    "        (pd.Series(importances_dt_pr, name=\"Precision-Score\") * 100),\n",
    "        (pd.Series(importances_dt_re, name=\"Recall-Score\") * 100),\n",
    "    ],\n",
    "    axis=1,\n",
    ").sort_values(by=\"AP-Score\", ascending=False)\n",
    "\n",
    "# Show Importances DataFrame\n",
    "display(importances_dt_df)\n",
    "\n",
    "# Show best trials\n",
    "best_trials_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Scores DataFrame\n",
    "score_dt_df = pd.DataFrame(\n",
    "    [trial.values for trial in best_trials_dt],\n",
    "    columns=[\"AP-Score\", \"Precision-Score\", \"Recall-Score\"],\n",
    "    index=[trial.number for trial in best_trials_dt],\n",
    ")\n",
    "# Rtnak the scores\n",
    "score_dt_df.loc[:, \"AP-Score-Rank\"] = (\n",
    "    score_dt_df.loc[:, \"AP-Score\"].rank(ascending=False, method=\"dense\").astype(int)\n",
    ")\n",
    "score_dt_df.loc[:, \"Precision-Score-Rank\"] = (\n",
    "    score_dt_df.loc[:, \"Precision-Score\"]\n",
    "    .rank(ascending=False, method=\"dense\")\n",
    "    .astype(int)\n",
    ")\n",
    "score_dt_df.loc[:, \"Recall-Score-Rank\"] = (\n",
    "    score_dt_df.loc[:, \"Recall-Score\"].rank(ascending=False, method=\"dense\").astype(int)\n",
    ")\n",
    "\n",
    "# The mean of all the ranks\n",
    "score_dt_df.loc[:, \"Mean-Rank\"] = (\n",
    "    score_dt_df.loc[:, [\"Precision-Score-Rank\", \"Recall-Score-Rank\"]]\n",
    "    .mean(axis=1)\n",
    "    .rank(ascending=True, method=\"dense\")\n",
    "    .astype(int)\n",
    ")\n",
    "# Show the Scores DataFrame\n",
    "display(score_dt_df.sort_values(by=\"Mean-Rank\"))\n",
    "\n",
    "# Filter the Scores DataFrame for Precision >= 70% and Recall >= 70%\n",
    "score_dt_df.loc[\n",
    "    (score_dt_df[\"Precision-Score\"] >= 0.70) & (score_dt_df[\"Recall-Score\"] >= 0.70)\n",
    "].sort_values(by=\"Mean-Rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Parameters for Trial Number 97\n",
    "best_dt_parms = [trial.params for trial in best_trials_dt if trial.number == 97][0]\n",
    "pprint(best_dt_parms)\n",
    "\n",
    "# Fit the Decision Tree with the best parameters\n",
    "best_dt = DecisionTreeClassifier(random_state=42)\n",
    "best_dt = best_dt.set_params(**best_dt_parms)\n",
    "\n",
    "# Get the classification report for best decision tree\n",
    "better_classification_report(best_dt, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fite the Decision Tree\n",
    "best_dt.fit(X_train, y_train)\n",
    "\n",
    "# Plot the tree\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "plot_tree(\n",
    "    best_dt,\n",
    "    filled=True,\n",
    "    class_names=[target_mapping[0], target_mapping[1]],\n",
    "    feature_names=best_dt.feature_names_in_,\n",
    "    max_depth=5,\n",
    "    fontsize=8,\n",
    "    ax=ax,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "best_dt.fit(X_train, y_train)\n",
    "\n",
    "# Crete matplotlib figure\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "# Plot the tree\n",
    "plot_tree(\n",
    "    best_dt,\n",
    "    feature_names=list(X_train.columns),\n",
    "    class_names=[\"Non-Fraud\", \"Fraud\"],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=8,\n",
    "    max_depth=4,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## *Random Forest*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Simple RF Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Show better classification report\n",
    "y_train_proba = better_classification_report(rf, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(y_train_proba[:, 1] >= 0.52, 1, 0)\n",
    "\n",
    "confusion_matrix(y_train, np.where(y_train_proba[:, 1] >= 0.52, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Precision-Recall for each threshold\n",
    "precision, recall, thresholds = precision_recall_curve(y_train, y_train_proba[:, 1])\n",
    "# Compute F1 for each threshold\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "# Select the best threshold with highest F1\n",
    "best_thresh = thresholds[np.argmax(f1)]\n",
    "# Print the best threshold\n",
    "print(f\"Best threshold for F1 = {best_thresh:.3f}\")\n",
    "\n",
    "# Precision Recall DataFrame\n",
    "pr_df = pd.DataFrame(\n",
    "    np.concat(\n",
    "        [\n",
    "            precision[:-1].reshape(-1, 1),\n",
    "            recall[:-1].reshape(-1, 1),\n",
    "            f1[:-1].reshape(-1, 1),\n",
    "            thresholds.reshape(-1, 1),\n",
    "        ],\n",
    "        axis=1,\n",
    "    ),\n",
    "    columns=[\"Precision\", \"Recall\", \"F1-Score\", \"Threshold\"],\n",
    ").sort_values([\"Recall\", \"Precision\"], ascending=[True, False])\n",
    "\n",
    "# Lets plot the Precision-Recall Curve\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=pr_df[\"Recall\"],\n",
    "        y=pr_df[\"Precision\"],\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"Precision-Recall Curve\",\n",
    "        line=dict(color=\"blue\", width=2),\n",
    "        # Add Custom data\n",
    "        customdata=pr_df[[\"F1-Score\", \"Threshold\"]].values,\n",
    "        # Add hover info\n",
    "        hovertemplate=(\n",
    "            \"Recall: %{x:.2%}<br>\"\n",
    "            \"Precision: %{y:.2%}<br>\"\n",
    "            \"F1: %{customdata[0]:.2%}<br>\"\n",
    "            \"Threshold: %{customdata[1]:.2%}\"\n",
    "            \"<extra></extra>\"\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add one Point for Max F1 Score\n",
    "max_f1_idx = pr_df[\"F1-Score\"].idxmax()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[pr_df.loc[max_f1_idx, \"Recall\"]],\n",
    "        y=[pr_df.loc[max_f1_idx, \"Precision\"]],\n",
    "        mode=\"markers+text\",\n",
    "        name=\"Max F1 Score Point\",\n",
    "        marker=dict(color=\"red\", size=10, symbol=\"circle\"),\n",
    "        text=[f\"Max F1: {pr_df.loc[max_f1_idx, 'F1-Score']:.2%}\"],\n",
    "        textposition=\"top center\",\n",
    "        hovertemplate=(\n",
    "            \"Recall: %{x:.2%}<br>\"\n",
    "            \"Precision: %{y:.2%}<br>\"\n",
    "            \"F1: %{text}<br>\"\n",
    "            \"Threshold: %{customdata[1]:.4%}\"\n",
    "            \"<extra></extra>\"\n",
    "        ),\n",
    "        customdata=np.array(\n",
    "            [[pr_df.loc[max_f1_idx, \"F1-Score\"], pr_df.loc[max_f1_idx, \"Threshold\"]]]\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update Layout\n",
    "fig.update_layout(\n",
    "    title=dict(text=\"<b>Precision-Recall Curve</b>\", x=0.5, font=dict(size=20)),\n",
    "    xaxis_title=\"<b>Recall</b>\",\n",
    "    yaxis_title=\"<b>Precision</b>\",\n",
    "    template=\"plotly_white\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the modle\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    pd.Series(rf.feature_importances_ * 100, index=rf.feature_names_in_).sort_values(\n",
    "        ascending=False\n",
    "    )\n",
    ")\n",
    "\n",
    "display(\n",
    "    pd.Series(rf.feature_importances_ * 100, index=rf.feature_names_in_)\n",
    "    .sort_values(ascending=False)\n",
    "    .cumsum()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Optimized RF Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up study and optimization for Random Forest\n",
    "study_rf = optuna.create_study(\n",
    "    # storage=db_path,\n",
    "    storage=None,\n",
    "    study_name=\"Random-Forest-V04\",\n",
    "    direction=\"maximize\",\n",
    "    load_if_exists=True,\n",
    ")\n",
    "\n",
    "# Set partial function for RF objective\n",
    "rf_objective: MultiObjectiveFunction = partial(\n",
    "    full_rf_multi_objective_func, X_train=X_train, y_train=y_train\n",
    ")\n",
    "\n",
    "# Start Optimization\n",
    "study_rf.optimize(\n",
    "    rf_objective,\n",
    "    n_trials=1000,\n",
    "    timeout=int(5 * 3600),\n",
    "    n_jobs=1,\n",
    "    gc_after_trial=True,\n",
    "    show_progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paramters in the decision tree\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "rf = rf.set_params(**study_rf.best_params)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "[tree.get_depth() for tree in rf.estimators_]\n",
    "\n",
    "# Create Figure Object\n",
    "fig = go.Figure()\n",
    "\n",
    "#  Draw the count of trees by depth\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=[tree.get_depth() for tree in rf.estimators_],\n",
    "        nbinsx=max([tree.get_depth() for tree in rf.estimators_]),\n",
    "        name=\"Count of Trees by Depth\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Feature Importances\n",
    "pd.Series(rf.feature_importances_ * 100, index=rf.feature_names_in_).sort_values(\n",
    "    ascending=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Parammeter Importance\n",
    "param_importance = get_param_importances(study_rf)\n",
    "\n",
    "# Show importtance\n",
    "display(pd.Series(param_importance) * 100)\n",
    "# Cumulative Importance\n",
    "display(pd.Series(param_importance).cumsum() * 100)\n",
    "# Plot the graph for the Cumulative Importance\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=pd.Series(param_importance).cumsum().index,\n",
    "        y=(pd.Series(param_importance) * 100).cumsum().values,\n",
    "        name=\"Cumulative Importance\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.Series(param_importance) * 100).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    (pd.Series(param_importance) * 100).cumsum().shift(-1)\n",
    "    - (pd.Series(param_importance) * 100).cumsum().shift(1)\n",
    ") / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get better classification report\n",
    "better_classification_report(rf, X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
